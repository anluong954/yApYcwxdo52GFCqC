{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Processing Tools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# embedding models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #TFiDF\n",
    "import torchtext #GloVe\n",
    "# from gensim.models import Word2Vec # Word2Vec\n",
    "import transformers #BERT and SBERT\n",
    "import torch\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Converty URL from google sheet into SCV\n",
    "# The export URL for CSV is: https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=csv&gid={GID}\n",
    "# Extracted from your URL:\n",
    "# - Spreadsheet ID: 117X6i53dKiO7w6kuA1g1TpdTlv1173h_dPlJt5cNNMU\n",
    "# - gid: 113676374"
   ],
   "id": "4e5ea19e64f8c2fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Exploration\n",
    "data = \"https://docs.google.com/spreadsheets/d/117X6i53dKiO7w6kuA1g1TpdTlv1173h_dPlJt5cNNMU/export?format=csv&gid=113676374\"\n",
    "df =pd.read_csv(data)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df.isnull().sum())\n",
    "print(df.duplicated().sum())\n",
    "df_copy = df.copy()"
   ],
   "id": "e4e008ca406069cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Processing data\n",
    "df_copy['job_title'].value_counts()\n",
    "abbreviations = {\n",
    "    'GPHR': 'Global Professional in Human Resources',\n",
    "    'CSR': 'Corporate Social Responsibility',\n",
    "    'MES': 'Manufacturing Execution Systems',\n",
    "    'SPHR': 'Senior Professional in Human Resources',\n",
    "    'SVP': 'Senior Vice President',\n",
    "    'GIS': 'Geographic Information System',\n",
    "    'RRP': 'Reduced Risk Products',\n",
    "    'CHRO': 'Chief Human Resources Officer',\n",
    "    'HRIS': 'Human resources information system',\n",
    "    'HR': 'Human resources',\n",
    "}\n",
    "def replace_abbreviations(title):\n",
    "    for k, v in abbreviations.items():\n",
    "        regex = r'\\b{}\\b'.format(re.escape(k))\n",
    "        title = re.sub(regex, v, title, flags=re.IGNORECASE)\n",
    "        return title\n",
    "\n",
    "def clean_title(title):\n",
    "    title = replace_abbreviations(title) #replace abbreviations\n",
    "    words = word_tokenize(title.lower()) # tokenize words\n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    stems = []\n",
    "    for words in words:\n",
    "        stem = ps.stem(words)\n",
    "        stems.append(stem)\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_stems = []\n",
    "    engl_stopwords = stopwords.words('english')\n",
    "    for stem in stems:\n",
    "        if stem not in engl_stopwords:\n",
    "            lemma = lemmatizer.lemmatize(stem)\n",
    "            lemmatized_stems.append(lemma)\n",
    "    return '_'.join(lemmatized_stems)\n",
    "\n",
    "df_copy['job_title'] = df_copy['job_title'].apply(clean_title)\n",
    "\n",
    "df_copy.head()\n",
    "\n",
    "df_copy.drop('fit', axis=1, inplace=True)\n",
    "keywords = ['aspiring human resources']\n",
    "data_master = df_copy.copy()"
   ],
   "id": "6a09c60847347211"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CBOW (Continuous Bag of Words\n",
    "word = re.compile(r'\\w+')\n",
    "def str_to_vec(str):\n",
    "    return Counter(word.findall(str))\n",
    "\n",
    "def get_cosine(v1, v2):\n",
    "    intersect = set(v1.keys()) & set(v2.keys())\n",
    "    numerator = sum([v1[i] * v2[i] for i in intersect])\n",
    "    sum_v1 = sum([v1[j] ** 2 for j in v1.keys()])\n",
    "    sum_v2 = sum([v2[k] ** 2 for k in v2.keys()])\n",
    "    denominator = math.sqrt(sum_v1) * math.sqrt(sum_v2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        cosine = float(numerator / denominator)\n",
    "        return cosine\n",
    "\n",
    "cbow_data = data_master.copy()\n",
    "cbow_title_embeddings = [str_to_vec(str) for str in cbow_data['job_title']]\n",
    "cbow_keywords_embeddings = [str_to_vec(str) for str in keywords]\n",
    "\n",
    "cbow_cosine = [get_cosine(key_emb, title_emb) for key_emb in cbow_keywords_embeddings for title_emb in cbow_title_embeddings]\n",
    "cbow_data['cbow_fit'] = cbow_cosine\n",
    "\n",
    "data = df.merge(cbow_data['cbow_fit'], how='left', left_index=True, right_index=True)\n",
    "data.sort_values('cbow_fit', ascending=False, inplace=True)\n",
    "data.head(10)"
   ],
   "id": "a5f00a81560fa1a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_data = data_master.copy()\n",
    "titles = tfidf_data['job_title'].tolist()\n",
    "\n",
    "tfidf_title_embs = vectorizer.fit_transform(titles)\n",
    "tfidf_keyword_embs = vectorizer.transform(keywords)\n",
    "\n",
    "tfidf_cosine = [cosine_similarity(tfidf_keyword_embs, tfidf_title_emb) for tfidf_title_emb in tfidf_title_embs]\n",
    "cosine_list = []\n",
    "for i in tfidf_cosine:\n",
    "  cosine_list.append(i.item())\n",
    "\n",
    "tfidf_data['tfidf_fit'] = cosine_list\n",
    "\n",
    "data = df.merge(tfidf_data['tfidf_fit'], how='left', left_index=True, right_index=True)\n",
    "data.sort_values('tfidf_fit', ascending=False, inplace=True)\n",
    "data.head(10)"
   ],
   "id": "cab8ef4aa4e05474"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GloVe\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "def str_to_glove(text):\n",
    "    # Split by underscore because clean_title joins with '_'\n",
    "    tokens = text.split('_')\n",
    "    ind = [glove.stoi[token] for token in tokens if token in glove.stoi]\n",
    "    if not ind:\n",
    "        return np.zeros(100)\n",
    "    vecs = glove.vectors[ind]\n",
    "    return vecs.numpy().mean(axis=0)\n",
    "\n",
    "glove_data = data_master.copy()\n",
    "glove_titles = glove_data['job_title'].apply(str_to_glove)\n",
    "cleaned_keywords = [clean_title(k) for k in keywords]\n",
    "glove_title_embeddings = np.stack(glove_titles.values)\n",
    "glove_keywords_embeddings = str_to_glove(cleaned_keywords[0]).reshape(1, -1)\n",
    "\n",
    "glove_cosines = cosine_similarity(glove_title_embeddings, glove_keywords_embeddings).flatten()\n",
    "glove_data['gloVe_fit'] = glove_cosines\n",
    "\n",
    "final_data = df.merge(glove_data[['gloVe_fit']], left_index=True, right_index=True)\n",
    "final_data.sort_values('gloVe_fit', ascending=False).head(10)"
   ],
   "id": "8212678fc8479e33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# BERT\n",
    "bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def str_to_bert_embedding(str):\n",
    "  ids = bert_tokenizer.encode_plus(str, add_special_tokens=True, return_tensors='pt')\n",
    "  out = bert(**ids)\n",
    "  embeddings = torch.mean(out.last_hidden_state, dim=1)\n",
    "  return embeddings\n",
    "\n",
    "bert_data = data_master.copy()\n",
    "bert_title_embeddings = [emb.detach().numpy() for emb in bert_data['job_title'].apply(str_to_bert_embedding)]\n",
    "bert_keywords_embeddings = str_to_bert_embedding(keywords[0]).detach().numpy()\n",
    "\n",
    "bert_cosine = [cosine_similarity(bert_keywords_embeddings, bert_title_embedding).item() for bert_title_embedding in bert_title_embeddings]\n",
    "bert_data['bert_fit'] = bert_cosine\n",
    "\n",
    "data = df.merge(bert_data['bert_fit'], how='left', left_index=True, right_index=True)\n",
    "data.sort_values('bert_fit', ascending=False, inplace=True)\n",
    "data.head(10)"
   ],
   "id": "de99d5efc3491fe1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SBERT\n",
    "sbert = transformers.AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "sbert_tokenizer = transformers.AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def str_to_sbert_embedding(str):\n",
    "  ids = sbert_tokenizer.encode_plus(str, add_special_tokens=True, return_tensors='pt')\n",
    "  out = sbert(**ids)\n",
    "  embeddings = torch.mean(out.last_hidden_state, dim=1)\n",
    "  return embeddings\n",
    "\n",
    "sbert_data = data_master.copy()\n",
    "sbert_title_embeddings = [emb.detach().numpy() for emb in sbert_data['job_title'].apply(str_to_sbert_embedding)]\n",
    "sbert_keywords_embeddings = str_to_sbert_embedding(keywords[0]).detach().numpy()\n",
    "\n",
    "sbert_cosine = [cosine_similarity(sbert_keywords_embeddings, sbert_title_embedding).item() for sbert_title_embedding in sbert_title_embeddings]\n",
    "sbert_data['sbert_fit'] = sbert_cosine\n",
    "\n",
    "data = df.merge(sbert_data['sbert_fit'], how='left', left_index=True, right_index=True)\n",
    "data.sort_values('sbert_fit', ascending=False, inplace=True)\n",
    "data.head(10)"
   ],
   "id": "21769fe0d95d3e31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Best method looks to be BERT\n",
    "# Reranking by update keyword string\n",
    "def update_keywords(keywords, candidate_ids, df):\n",
    "  for i in candidate_ids:\n",
    "    keywords_join = ' '.join(keywords)\n",
    "    keywords_l = keywords_join.lower().split()\n",
    "    job_titles = df.loc[i]['job_title'].lower().split()\n",
    "    for title in job_titles:\n",
    "      if title not in keywords_l:\n",
    "        keywords_l.append(title)\n",
    "        keywords = ' '.join(keywords_l)\n",
    "  return keywords\n",
    "\n",
    "rerank_data = data_master.copy()\n",
    "candidate_id = rerank_data.index.tolist()[0:10]\n",
    "updated_keywords = update_keywords(keywords, candidate_id, rerank_data)\n",
    "\n",
    "bert_updated_keywords_embeddings = str_to_bert_embedding(updated_keywords).detach().numpy()\n",
    "\n",
    "bert_cosine_reranked = [cosine_similarity(bert_updated_keywords_embeddings, bert_title_embedding).item() for bert_title_embedding in bert_title_embeddings]\n",
    "\n",
    "rerank_data['rerank_bert_fit'] = bert_cosine_reranked\n",
    "\n",
    "data = data.merge(rerank_data['rerank_bert_fit'], how='left', left_index=True, right_index=True)\n",
    "data.sort_values('rerank_bert_fit', ascending=False, inplace=True)\n",
    "print(data.head(10))"
   ],
   "id": "f240c2a645dd92f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reranking via embedding averages\n",
    "def get_candidate_keywords(candidate_id, df):\n",
    "  candidate_keywords_l = []\n",
    "  for i in candidate_id:\n",
    "    candidate_title = df.loc[i]['job_title'].lower().split()\n",
    "    for word in candidate_title:\n",
    "      if word not in candidate_keywords_l:\n",
    "        candidate_keywords_l.append(word)\n",
    "  candidate_keywords = ' '.join(candidate_keywords_l)\n",
    "  return candidate_keywords\n",
    "\n",
    "def averaged_bert_emb(keywords, candidate_id, df):\n",
    "  bert_keywords_embeddings = str_to_bert_embedding(keywords)\n",
    "\n",
    "  candidate_keywords = get_candidate_keywords(candidate_id, df)\n",
    "  bert_candidate_keywords = str_to_bert_embedding(candidate_keywords)\n",
    "\n",
    "  avg_bert_emb = (bert_keywords_embeddings + bert_candidate_keywords)/2\n",
    "  return avg_bert_emb\n",
    "\n",
    "rerank2_data = data_master.copy()\n",
    "candidate_id = rerank_data.index.tolist()[0:10]\n",
    "\n",
    "updated_avg_keywords = averaged_bert_emb(keywords, candidate_id, rerank2_data).detach().numpy()\n",
    "\n",
    "bert_cosine_reranked_2 = [cosine_similarity(updated_avg_keywords, bert_title_embedding).item() for bert_title_embedding in bert_title_embeddings]\n",
    "\n",
    "rerank2_data['rerank2_bert_fit'] = bert_cosine_reranked_2\n",
    "\n",
    "data = data.merge(rerank2_data['rerank2_bert_fit'], how='left', left_index=True, right_index=True)\n",
    "data.sort_values('rerank2_bert_fit', ascending=False, inplace=True)\n",
    "print(data.head(10))"
   ],
   "id": "7cf33d5c99a2a227"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
